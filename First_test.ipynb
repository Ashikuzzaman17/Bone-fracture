{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13521879,
          "sourceType": "datasetVersion",
          "datasetId": 8585831
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "First-test",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashikuzzaman17/Bone-fracture/blob/main/First_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "ashikuzzamanshishir_bone_fracture_dataset_path = kagglehub.dataset_download('ashikuzzamanshishir/bone-fracture-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOLdlsb4AV1F",
        "outputId": "6fa71692-e1c2-42d7-c65f-82de6a6fc431"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'bone-fracture-dataset' dataset.\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddedbc29",
        "outputId": "bc7a47a4-83a1-4a05-b743-e98b984c4f3b"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 1 – Imports & global constants\n",
        "# ----------------------------------------------------------\n",
        "import os, random, time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Deep‑learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, callbacks, regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.applications import (\n",
        "    ResNet50, VGG16, InceptionV3, MobileNetV2, EfficientNetB0\n",
        ")\n",
        "\n",
        "# Transformers (timm)\n",
        "import timm\n",
        "\n",
        "# Image quality metrics\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "\n",
        "# OpenCV for CLAHE\n",
        "import cv2\n",
        "\n",
        "# Misc\n",
        "import optuna\n",
        "import json\n",
        "import psutil  # for memory usage\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:13.043911Z",
          "iopub.execute_input": "2025-10-28T13:27:13.044306Z",
          "iopub.status.idle": "2025-10-28T13:27:13.052056Z",
          "shell.execute_reply.started": "2025-10-28T13:27:13.044271Z",
          "shell.execute_reply": "2025-10-28T13:27:13.05099Z"
        },
        "id": "JYKsdx52AV1G"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 2 – Define dataset root and list sub‑folders\n",
        "# ----------------------------------------------------------\n",
        "DATASET_ROOT = Path('/kaggle/input/bone-fracture-dataset/Bone fracture dataset/Dataset')  # <-- change this\n",
        "\n",
        "# Count sub‑folders (classes)\n",
        "class_names = [d.name for d in DATASET_ROOT.iterdir() if d.is_dir()]\n",
        "n_classes = len(class_names)\n",
        "print(f'Found {n_classes} classes:', class_names)\n",
        "\n",
        "# Count total images\n",
        "total_imgs = sum(len([f for f in p.iterdir() if f.is_file()]) for p in DATASET_ROOT.glob('*'))\n",
        "print(f'Total images: {total_imgs}')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:13.054014Z",
          "iopub.execute_input": "2025-10-28T13:27:13.054297Z",
          "iopub.status.idle": "2025-10-28T13:27:17.462668Z",
          "shell.execute_reply.started": "2025-10-28T13:27:13.054275Z",
          "shell.execute_reply": "2025-10-28T13:27:17.461419Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3Jqwt5rAV1G",
        "outputId": "4c540e01-a2a1-4233-b3e1-eb2ee320fd09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 classes: ['normal', 'fracture']\n",
            "Total images: 2127\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 3 – CLAHE + optional Gaussian blur + rescale\n",
        "# ----------------------------------------------------------\n",
        "def preprocess_img(img_path, target_size=(256,256)):\n",
        "    \"\"\"\n",
        "    Load image, apply CLAHE, optional Gaussian blur,\n",
        "    resize, and rescale to [0,1].\n",
        "    \"\"\"\n",
        "    # Load\n",
        "    img = cv2.imread(str(img_path))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # CLAHE on each channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    img = cv2.merge([clahe.apply(c) for c in cv2.split(img)])\n",
        "\n",
        "    # Optional Gaussian blur\n",
        "    # img = cv2.GaussianBlur(img, (3,3), 0)\n",
        "\n",
        "    # Resize\n",
        "    img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Rescale to [0,1]\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return img\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.463933Z",
          "iopub.execute_input": "2025-10-28T13:27:17.464295Z",
          "iopub.status.idle": "2025-10-28T13:27:17.472692Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.464266Z",
          "shell.execute_reply": "2025-10-28T13:27:17.471415Z"
        },
        "id": "KV574hgxAV1G"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell 4 – PSNR / SSIM (Kaggle auto‑path) ----------------\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "from pathlib import Path\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# 1️⃣  Pre‑process an image: resize (optional), convert to RGB, normalise\n",
        "#    (Using the preprocess_img from Cell 3)\n",
        "# --------------------------------------------------------------------------- #\n",
        "def preprocess_img(img_path, target_size=(256,256)):\n",
        "    \"\"\"\n",
        "    Load image, apply CLAHE, optional Gaussian blur,\n",
        "    resize, and rescale to [0,1].\n",
        "    \"\"\"\n",
        "    # Load\n",
        "    img = cv2.imread(str(img_path))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # CLAHE on each channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    img = cv2.merge([clahe.apply(c) for c in cv2.split(img)])\n",
        "\n",
        "    # Optional Gaussian blur\n",
        "    # img = cv2.GaussianBlur(img, (3,3), 0)\n",
        "\n",
        "    # Resize\n",
        "    img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Rescale to [0,1]\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# 2️⃣  PSNR & SSIM – auto‑resize processed image if shapes differ\n",
        "# --------------------------------------------------------------------------- #\n",
        "def psnr_ssim(original: np.ndarray,\n",
        "              processed: np.ndarray,\n",
        "              eps: float = 1e-10) -> tuple[float, float]:\n",
        "    original = original.astype(np.float32)\n",
        "    processed = processed.astype(np.float32)\n",
        "\n",
        "    if original.shape != processed.shape:\n",
        "        processed = cv2.resize(processed,\n",
        "                               (original.shape[1], original.shape[0]),\n",
        "                               interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    if original.shape != processed.shape:\n",
        "        raise ValueError(f\"Shape mismatch after resizing: {original.shape} vs {processed.shape}\")\n",
        "\n",
        "    # Ensure images are grayscale before calculating SSIM if they are not already\n",
        "    if original.ndim == 3 and original.shape[-1] == 3:\n",
        "      original_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY)\n",
        "      processed_gray = cv2.cvtColor(processed, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "      original_gray = original\n",
        "      processed_gray = processed\n",
        "\n",
        "\n",
        "    psnr_val = peak_signal_noise_ratio(original, processed, data_range=1.0)\n",
        "    ssim_val = structural_similarity(original_gray,\n",
        "                                      processed_gray,\n",
        "                                      multichannel=False, # Set to False for grayscale\n",
        "                                      data_range=1.0,\n",
        "                                      eps=eps)\n",
        "    return psnr_val, ssim_val\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# 3️⃣  Find first image folder under /kaggle/input (auto‑detect)\n",
        "# --------------------------------------------------------------------------- #\n",
        "def find_kaggle_image_folder() -> Path:\n",
        "    \"\"\"Return the first sub‑folder of /kaggle/input that contains images.\"\"\"\n",
        "    base = Path(\"/kaggle/input\")\n",
        "    if not base.is_dir():\n",
        "        raise FileNotFoundError(\"Kaggle input directory not found\")\n",
        "\n",
        "    for candidate in base.iterdir():\n",
        "        if candidate.is_dir():\n",
        "            # quick check if it has any image files\n",
        "            imgs = list(candidate.glob(\"*.jpg\")) + \\\n",
        "                   list(candidate.glob(\"*.jpeg\")) + \\\n",
        "                   list(candidate.glob(\"*.png\"))\n",
        "            if imgs:\n",
        "                return candidate\n",
        "\n",
        "    raise FileNotFoundError(\"No image folder found under /kaggle/input\")\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# 4️⃣  Run PSNR/SSIM on all images in the detected folder\n",
        "# --------------------------------------------------------------------------- #\n",
        "def demo_folder(folder: Path) -> None:\n",
        "    print(f\"Scanning folder: {folder}\")\n",
        "    for img_file in folder.rglob(\"*\"):\n",
        "        if img_file.suffix.lower() not in {'.jpg', '.jpeg', '.png'}:\n",
        "            continue\n",
        "\n",
        "        # Raw image\n",
        "        raw = cv2.imread(str(img_file))\n",
        "        raw_rgb = cv2.cvtColor(raw, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "        # Pre‑processed image using the function from Cell 3\n",
        "        pre = preprocess_img(img_file,\n",
        "                             target_size=(raw_rgb.shape[1], raw_rgb.shape[0]))\n",
        "\n",
        "        psnr_val, ssim_val = psnr_ssim(raw_rgb, pre)\n",
        "        print(f\"{img_file.name:30} -> PSNR = {psnr_val:.2f} dB, SSIM = {ssim_val:.3f}\")\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# 5️⃣  Execute the demo automatically (press *Run* to start)\n",
        "# --------------------------------------------------------------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    # Use the known dataset path directly\n",
        "    folder_path = Path('/kaggle/input/bone-fracture-dataset/Bone fracture dataset/Dataset')\n",
        "    demo_folder(folder_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.473631Z",
          "iopub.execute_input": "2025-10-28T13:27:17.473932Z",
          "iopub.status.idle": "2025-10-28T13:27:17.554657Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.47391Z",
          "shell.execute_reply": "2025-10-28T13:27:17.552175Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YawseEXUAV1G",
        "outputId": "c5ab62f6-9260-4405-8986-a227422a56d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning folder: /kaggle/input/bone-fracture-dataset/Bone fracture dataset/Dataset\n",
            "379.png                        -> PSNR = 26.96 dB, SSIM = 0.954\n",
            "340.png                        -> PSNR = 24.74 dB, SSIM = 0.935\n",
            "48.png                         -> PSNR = 25.73 dB, SSIM = 0.946\n",
            "227.png                        -> PSNR = 24.25 dB, SSIM = 0.938\n",
            "61.png                         -> PSNR = 22.64 dB, SSIM = 0.915\n",
            "377.png                        -> PSNR = 26.96 dB, SSIM = 0.954\n",
            "222.png                        -> PSNR = 24.25 dB, SSIM = 0.938\n",
            "37.png                         -> PSNR = 24.67 dB, SSIM = 0.925\n",
            "35.png                         -> PSNR = 24.40 dB, SSIM = 0.967\n",
            "384.png                        -> PSNR = 25.03 dB, SSIM = 0.880\n",
            "376.png                        -> PSNR = 24.67 dB, SSIM = 0.973\n",
            "133.png                        -> PSNR = 23.48 dB, SSIM = 0.955\n",
            "70.png                         -> PSNR = 23.90 dB, SSIM = 0.934\n",
            "73.png                         -> PSNR = 25.12 dB, SSIM = 0.903\n",
            "92.png                         -> PSNR = 23.72 dB, SSIM = 0.962\n",
            "204.png                        -> PSNR = 23.93 dB, SSIM = 0.888\n",
            "75.png                         -> PSNR = 23.90 dB, SSIM = 0.934\n",
            "89.png                         -> PSNR = 20.89 dB, SSIM = 0.917\n",
            "338.png                        -> PSNR = 23.88 dB, SSIM = 0.929\n",
            "344.png                        -> PSNR = 23.88 dB, SSIM = 0.929\n",
            "76.png                         -> PSNR = 23.88 dB, SSIM = 0.896\n",
            "138.png                        -> PSNR = 22.79 dB, SSIM = 0.947\n",
            "225.png                        -> PSNR = 22.92 dB, SSIM = 0.930\n",
            "43.png                         -> PSNR = 22.55 dB, SSIM = 0.927\n",
            "345.png                        -> PSNR = 25.83 dB, SSIM = 0.944\n",
            "40.png                         -> PSNR = 25.73 dB, SSIM = 0.946\n",
            "209.png                        -> PSNR = 24.07 dB, SSIM = 0.899\n",
            "85.png                         -> PSNR = 20.89 dB, SSIM = 0.917\n",
            "9.png                          -> PSNR = 22.79 dB, SSIM = 0.947\n",
            "56.png                         -> PSNR = 24.99 dB, SSIM = 0.922\n",
            "80.png                         -> PSNR = 23.90 dB, SSIM = 0.934\n",
            "55.png                         -> PSNR = 26.11 dB, SSIM = 0.947\n",
            "378.png                        -> PSNR = 24.67 dB, SSIM = 0.973\n",
            "50.png                         -> PSNR = 24.99 dB, SSIM = 0.922\n",
            "49.png                         -> PSNR = 22.55 dB, SSIM = 0.927\n",
            "93.png                         -> PSNR = 23.72 dB, SSIM = 0.962\n",
            "59.png                         -> PSNR = 23.77 dB, SSIM = 0.930\n",
            "136.png                        -> PSNR = 22.53 dB, SSIM = 0.963\n",
            "14.png                         -> PSNR = 21.54 dB, SSIM = 0.947\n",
            "65.png                         -> PSNR = 23.77 dB, SSIM = 0.930\n",
            "371.png                        -> PSNR = 24.67 dB, SSIM = 0.973\n",
            "1.png                          -> PSNR = 23.48 dB, SSIM = 0.955\n",
            "39.png                         -> PSNR = 26.11 dB, SSIM = 0.947\n",
            "380.png                        -> PSNR = 25.03 dB, SSIM = 0.880\n",
            "370.png                        -> PSNR = 25.87 dB, SSIM = 0.970\n",
            "372.png                        -> PSNR = 25.03 dB, SSIM = 0.880\n",
            "90.png                         -> PSNR = 23.72 dB, SSIM = 0.962\n",
            "20.png                         -> PSNR = 23.03 dB, SSIM = 0.965\n",
            "83.png                         -> PSNR = 20.89 dB, SSIM = 0.917\n",
            "38.png                         -> PSNR = 25.73 dB, SSIM = 0.946\n",
            "335.png                        -> PSNR = 25.83 dB, SSIM = 0.944\n",
            "223.png                        -> PSNR = 22.92 dB, SSIM = 0.930\n",
            "82.png                         -> PSNR = 22.64 dB, SSIM = 0.945\n",
            "36.png                         -> PSNR = 25.73 dB, SSIM = 0.946\n",
            "221.png                        -> PSNR = 24.25 dB, SSIM = 0.938\n",
            "383.png                        -> PSNR = 25.87 dB, SSIM = 0.970\n",
            "208.png                        -> PSNR = 24.07 dB, SSIM = 0.899\n",
            "137.png                        -> PSNR = 22.79 dB, SSIM = 0.947\n",
            "81.png                         -> PSNR = 23.90 dB, SSIM = 0.934\n",
            "66.png                         -> PSNR = 22.64 dB, SSIM = 0.915\n",
            "58.png                         -> PSNR = 27.10 dB, SSIM = 0.955\n",
            "339.png                        -> PSNR = 23.88 dB, SSIM = 0.929\n",
            "336.png                        -> PSNR = 23.88 dB, SSIM = 0.929\n",
            "18.png                         -> PSNR = 21.54 dB, SSIM = 0.947\n",
            "63.png                         -> PSNR = 22.64 dB, SSIM = 0.915\n",
            "21.png                         -> PSNR = 21.54 dB, SSIM = 0.947\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 5 – Split 80/10/10 with tf.data\n",
        "# ----------------------------------------------------------\n",
        "IMG_SIZE = (256, 256)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    DATASET_ROOT,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=42,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "val_test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    DATASET_ROOT,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=42,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "# Split val_test into val and test (50/50)\n",
        "val_ds = val_test_ds.take(int(0.5 * len(val_test_ds)))\n",
        "test_ds = val_test_ds.skip(int(0.5 * len(val_test_ds)))\n",
        "\n",
        "print('Training batches:', len(train_ds))\n",
        "print('Validation batches:', len(val_ds))\n",
        "print('Testing batches:', len(test_ds))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.555494Z",
          "iopub.status.idle": "2025-10-28T13:27:17.555931Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.555724Z",
          "shell.execute_reply": "2025-10-28T13:27:17.555741Z"
        },
        "id": "eB08UfSHAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 6 – Data augmentation – two versions per image\n",
        "# ----------------------------------------------------------\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Custom generator that yields 3 images per original: (orig, aug1, aug2)\n",
        "def augment_generator(ds, augmentations=2):\n",
        "    for batch, labels in ds:\n",
        "        batch_aug = []\n",
        "        for img in batch:\n",
        "            batch_aug.append(img.numpy())   # original\n",
        "            for _ in range(augmentations):\n",
        "                aug_img = aug.random_transform(img.numpy())\n",
        "                batch_aug.append(aug_img)\n",
        "        # reshape to (batch_size*(1+aug), H, W, C)\n",
        "        batch_aug = np.stack(batch_aug, axis=0)\n",
        "        # Shuffle to mix originals with aug\n",
        "        idx = np.arange(len(batch_aug))\n",
        "        np.random.shuffle(idx)\n",
        "        yield batch_aug[idx], np.repeat(labels.numpy(), augmentations+1, axis=0)\n",
        "\n",
        "# Wrap in tf.data\n",
        "train_aug_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: augment_generator(train_ds),\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=(\n",
        "        (None, IMG_SIZE[0], IMG_SIZE[1], 3),\n",
        "        (None, n_classes)\n",
        "    )\n",
        ").batch(BATCH_SIZE * (1+2)).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# For validation and test we keep originals only\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.558745Z",
          "iopub.status.idle": "2025-10-28T13:27:17.559188Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.559039Z",
          "shell.execute_reply": "2025-10-28T13:27:17.559057Z"
        },
        "id": "ayF3_GEAAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 7 – Build a fine‑tuned CNN model\n",
        "# ----------------------------------------------------------\n",
        "def build_cnn(base_name='ResNet50'):\n",
        "    \"\"\"\n",
        "    Returns a compiled model.\n",
        "    base_name: one of ['ResNet50', 'VGG16', 'InceptionV3',\n",
        "                       'MobileNetV2', 'EfficientNetB0']\n",
        "    \"\"\"\n",
        "    base_model = {\n",
        "        'ResNet50': ResNet50,\n",
        "        'VGG16': VGG16,\n",
        "        'InceptionV3': InceptionV3,\n",
        "        'MobileNetV2': MobileNetV2,\n",
        "        'EfficientNetB0': EfficientNetB0\n",
        "    }[base_name](weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(*IMG_SIZE, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.5, rate=0.5),\n",
        "        layers.Dense(256, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', Precision(name='prec'), Recall(name='rec')]\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.561087Z",
          "iopub.status.idle": "2025-10-28T13:27:17.561481Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.561286Z",
          "shell.execute_reply": "2025-10-28T13:27:17.561299Z"
        },
        "id": "hCi5T5pjAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 8 – Train multiple CNNs with early stopping\n",
        "# ----------------------------------------------------------\n",
        "cnn_models = ['ResNet50', 'VGG16', 'InceptionV3',\n",
        "              'MobileNetV2', 'EfficientNetB0', 'ResNet50', 'VGG16']\n",
        "\n",
        "history_dict = {}\n",
        "train_start = time.time()\n",
        "\n",
        "for name in cnn_models:\n",
        "    print(f'\\n=== Training {name} ===')\n",
        "    model = build_cnn(name)\n",
        "\n",
        "    es = callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n",
        "    csv = callbacks.CSVLogger(f'{name}_history.csv')\n",
        "\n",
        "    history = model.fit(\n",
        "        train_aug_ds,\n",
        "        epochs=40,\n",
        "        validation_data=val_ds,\n",
        "        callbacks=[es, csv]\n",
        "    )\n",
        "    history_dict[name] = history.history\n",
        "    model.save(f'{name}_final.h5')\n",
        "\n",
        "    print(f'{name} training time: {time.time() - train_start:.1f}s')\n",
        "\n",
        "print('All CNN training finished.')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.562762Z",
          "iopub.status.idle": "2025-10-28T13:27:17.563213Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.562981Z",
          "shell.execute_reply": "2025-10-28T13:27:17.562999Z"
        },
        "id": "qUyoSQuDAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 9 – Build a fine‑tuned transformer (timm)\n",
        "# ----------------------------------------------------------\n",
        "def build_transformer(model_name='vit_base_patch16_224', lr=1e-4):\n",
        "    \"\"\"\n",
        "    Build a timm model for image classification.\n",
        "    model_name: e.g., 'swin_tiny_patch4_window7_224', 'vit_tiny', 'levit_base', etc.\n",
        "    \"\"\"\n",
        "    model = timm.create_model(\n",
        "        model_name,\n",
        "        pretrained=True,\n",
        "        num_classes=n_classes,\n",
        "        global_pool='avg'  # default\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    # Freeze backbone\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'head' not in name:   # freeze everything except classifier\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Wrap in Keras\n",
        "    class TimmModel(tf.keras.Model):\n",
        "        def __init__(self, timm_model):\n",
        "            super().__init__()\n",
        "            self.timm = timm_model\n",
        "        def call(self, x):\n",
        "            return self.timm(x)\n",
        "\n",
        "    tf_model = TimmModel(model)\n",
        "\n",
        "    tf_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', Precision(name='prec'), Recall(name='rec')]\n",
        "    )\n",
        "    return tf_model\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.565066Z",
          "iopub.status.idle": "2025-10-28T13:27:17.565626Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.565424Z",
          "shell.execute_reply": "2025-10-28T13:27:17.565442Z"
        },
        "id": "8kyyJHSzAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 10 – Train a few transformer variants\n",
        "# ----------------------------------------------------------\n",
        "transformer_names = [\n",
        "    'swin_tiny_patch4_window7_224',\n",
        "    'vit_tiny_patch16_224',\n",
        "    'levit_base',\n",
        "    'deit_tiny_distilled_patch16_224',\n",
        "    'mobilevit_s'\n",
        "]\n",
        "\n",
        "transform_history = {}\n",
        "train_start = time.time()\n",
        "\n",
        "for t_name in transformer_names:\n",
        "    print(f'\\n=== Training {t_name} ===')\n",
        "    model = build_transformer(t_name, lr=1e-4)\n",
        "\n",
        "    es = callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n",
        "    csv = callbacks.CSVLogger(f'{t_name}_history.csv')\n",
        "\n",
        "    history = model.fit(\n",
        "        train_aug_ds,\n",
        "        epochs=40,\n",
        "        validation_data=val_ds,\n",
        "        callbacks=[es, csv]\n",
        "    )\n",
        "    transform_history[t_name] = history.history\n",
        "    model.save(f'{t_name}_final.h5')\n",
        "\n",
        "    print(f'{t_name} training time: {time.time() - train_start:.1f}s')\n",
        "\n",
        "print('All transformer training finished.')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.5674Z",
          "iopub.status.idle": "2025-10-28T13:27:17.567758Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.567623Z",
          "shell.execute_reply": "2025-10-28T13:27:17.567638Z"
        },
        "id": "m-dvxVB9AV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 11 – Add CBAM attention and re‑train selected models\n",
        "# ----------------------------------------------------------\n",
        "# CBAM from https://github.com/Jongchan/attention-module\n",
        "# Here we illustrate it for ResNet50 as an example.\n",
        "\n",
        "def cbam_module(input_feature, reduction_ratio=16):\n",
        "    # Channel Attention\n",
        "    channel = layers.GlobalAveragePooling2D()(input_feature)\n",
        "    channel = layers.Dense(channel.shape[-1]//reduction_ratio,\n",
        "                           activation='relu',\n",
        "                           use_bias=True,\n",
        "                           kernel_initializer='he_normal')(channel)\n",
        "    channel = layers.Dense(channel.shape[-1],\n",
        "                           activation='sigmoid',\n",
        "                           use_bias=True,\n",
        "                           kernel_initializer='he_normal')(channel)\n",
        "    channel = layers.Reshape((1,1,channel.shape[-1]))(channel)\n",
        "    channel_attention = layers.Multiply()([input_feature, channel])\n",
        "\n",
        "    # Spatial Attention\n",
        "    spatial = layers.Conv2D(1, kernel_size=7,\n",
        "                            padding='same',\n",
        "                            activation='sigmoid',\n",
        "                            use_bias=False)(channel_attention)\n",
        "    return layers.Multiply()([channel_attention, spatial])\n",
        "\n",
        "# Example: Insert CBAM after base_model in CNN\n",
        "def build_cnn_cbam(base_name='ResNet50'):\n",
        "    base_model = {\n",
        "        'ResNet50': ResNet50,\n",
        "        'VGG16': VGG16,\n",
        "        'InceptionV3': InceptionV3,\n",
        "        'MobileNetV2': MobileNetV2,\n",
        "        'EfficientNetB0': EfficientNetB0\n",
        "    }[base_name](weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(*IMG_SIZE, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.Lambda(lambda x: cbam_module(x)),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Uncomment to re‑train one model with CBAM\n",
        "# model_cbam = build_cnn_cbam('ResNet50')\n",
        "# ... fit as above\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.568536Z",
          "iopub.status.idle": "2025-10-28T13:27:17.568811Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.568681Z",
          "shell.execute_reply": "2025-10-28T13:27:17.568695Z"
        },
        "id": "1tKa-lHMAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 12 – Evaluate on test set\n",
        "# ----------------------------------------------------------\n",
        "def evaluate_model(model_path):\n",
        "    model = tf.keras.models.load_model(model_path,\n",
        "                                       custom_objects={'CBAM': cbam_module})   # if using CBAM\n",
        "    loss, acc, prec, rec = model.evaluate(test_ds, verbose=0)\n",
        "    print(f'{model_path} - Acc:{acc:.4f} Prec:{prec:.4f} Rec:{rec:.4f}')\n",
        "    return acc, prec, rec\n",
        "\n",
        "# Pick the best CNN (by val accuracy)\n",
        "best_cnn_name = max(history_dict, key=lambda k: max(history_dict[k]['val_accuracy']))\n",
        "print('\\nBest CNN:', best_cnn_name)\n",
        "evaluate_model(f'{best_cnn_name}_final.h5')\n",
        "\n",
        "# Pick the best Transformer\n",
        "best_tf_name = max(transform_history, key=lambda k: max(transform_history[k]['val_accuracy']))\n",
        "print('\\nBest Transformer:', best_tf_name)\n",
        "evaluate_model(f'{best_tf_name}_final.h5')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.570385Z",
          "iopub.status.idle": "2025-10-28T13:27:17.570658Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.570532Z",
          "shell.execute_reply": "2025-10-28T13:27:17.570543Z"
        },
        "id": "yGtwWUkCAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 13 – Confusion matrix + classification report\n",
        "# ----------------------------------------------------------\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "def get_predictions(ds):\n",
        "    preds = []\n",
        "    truths = []\n",
        "    for batch, labels in ds:\n",
        "        probs = model.predict(batch)\n",
        "        preds.extend(np.argmax(probs, axis=-1))\n",
        "        truths.extend(np.argmax(labels, axis=-1))\n",
        "    return np.array(preds), np.array(truths)\n",
        "\n",
        "# Example for the best CNN\n",
        "model = tf.keras.models.load_model(f'{best_cnn_name}_final.h5')\n",
        "preds, truths = get_predictions(test_ds)\n",
        "\n",
        "conf_mat = skm.confusion_matrix(truths, preds)\n",
        "print('Confusion matrix:\\n', conf_mat)\n",
        "\n",
        "print('Classification report:')\n",
        "print(skm.classification_report(truths, preds, target_names=train_ds.class_names))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.572278Z",
          "iopub.status.idle": "2025-10-28T13:27:17.572583Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.572458Z",
          "shell.execute_reply": "2025-10-28T13:27:17.572471Z"
        },
        "id": "OHhNSQxBAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 14 – Summary\n",
        "# ----------------------------------------------------------\n",
        "def layer_summary(model):\n",
        "    model.build((None, *IMG_SIZE, 3))\n",
        "    model.summary()\n",
        "    trainable_count = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
        "    print('Trainable params:', trainable_count)\n",
        "\n",
        "layer_summary(best_cnn_name)\n",
        "layer_summary(best_tf_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.573948Z",
          "iopub.status.idle": "2025-10-28T13:27:17.574231Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.574095Z",
          "shell.execute_reply": "2025-10-28T13:27:17.574113Z"
        },
        "id": "KsNY_8McAV1H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 15 – Detailed metric plot (accuracy, precision, recall)\n",
        "# ----------------------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(hist, title='Model History'):\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(hist['accuracy'], label='train')\n",
        "    plt.plot(hist['val_accuracy'], label='val')\n",
        "    plt.title(f'{title} Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(hist['prec'], label='train')\n",
        "    plt.plot(hist['val_prec'], label='val')\n",
        "    plt.title(f'{title} Precision')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for name, hist in history_dict.items():\n",
        "    plot_history(hist, name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.57601Z",
          "iopub.status.idle": "2025-10-28T13:27:17.576403Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.576182Z",
          "shell.execute_reply": "2025-10-28T13:27:17.576198Z"
        },
        "id": "sDsOcY9mAV1I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 16 – Convert Keras H5 to ONNX (tf2onnx)\n",
        "# ----------------------------------------------------------\n",
        "!pip install tf2onnx\n",
        "\n",
        "import tf2onnx\n",
        "\n",
        "def export_to_onnx(h5_path, onnx_path):\n",
        "    model = tf.keras.models.load_model(h5_path)\n",
        "    spec = (tf.TensorSpec((None, *IMG_SIZE, 3), tf.float32, name=\"input\"),)\n",
        "    output_path = onnx_path\n",
        "    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec,\n",
        "                                                output_path=output_path)\n",
        "    print(f'Exported {h5_path} to {onnx_path}')\n",
        "\n",
        "export_to_onnx(f'{best_cnn_name}_final.h5', f'{best_cnn_name}.onnx')\n",
        "export_to_onnx(f'{best_tf_name}_final.h5', f'{best_tf_name}.onnx')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.577715Z",
          "iopub.status.idle": "2025-10-28T13:27:17.578114Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.577914Z",
          "shell.execute_reply": "2025-10-28T13:27:17.57793Z"
        },
        "id": "StRG3GM9AV1I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 17 – Create an HTML summary page\n",
        "# ----------------------------------------------------------\n",
        "summary = f\"\"\"\n",
        "# Model Summary – {best_cnn_name} vs {best_tf_name}\n",
        "\n",
        "| Metric | CNN ({best_cnn_name}) | Transformer ({best_tf_name}) |\n",
        "|--------|----------------------|------------------------------|\n",
        "\"\"\"\n",
        "\n",
        "for metric in ['accuracy', 'prec', 'rec']:\n",
        "    cnn_acc = max(history_dict[best_cnn_name]['val_accuracy'])\n",
        "    tf_acc = max(transform_history[best_tf_name]['val_accuracy'])\n",
        "    summary += f\"| {metric} | {cnn_acc:.4f} | {tf_acc:.4f} |\\n\"\n",
        "\n",
        "# Add confusion matrices and other notes\n",
        "summary += \"\\n*See individual Jupyter cells for training history, PSNR/SSIM, and confusion matrices.*\"\n",
        "\n",
        "open('model_summary.md', 'w').write(summary)\n",
        "print('Model summary written to model_summary.md')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.580098Z",
          "iopub.status.idle": "2025-10-28T13:27:17.581135Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.580426Z",
          "shell.execute_reply": "2025-10-28T13:27:17.580457Z"
        },
        "id": "54febrD7AV1I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------\n",
        "# Cell 18 – Final timing & memory usage\n",
        "# ----------------------------------------------------------\n",
        "train_end = time.time()\n",
        "print(f'Total training time: {train_end - train_start:.1f}s')\n",
        "\n",
        "# Memory usage\n",
        "import psutil\n",
        "process = psutil.Process()\n",
        "print('RAM used (MB):', process.memory_info().rss / (1024**2))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-28T13:27:17.582695Z",
          "iopub.status.idle": "2025-10-28T13:27:17.582999Z",
          "shell.execute_reply.started": "2025-10-28T13:27:17.582873Z",
          "shell.execute_reply": "2025-10-28T13:27:17.582885Z"
        },
        "id": "G-Wvho2MAV1I"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}